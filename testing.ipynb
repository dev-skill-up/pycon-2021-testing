{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing Unit Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teacher: [Moshe Zadka](https://cobordism.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start time: **15:00 US/Eastern**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgement of Country\n",
    "\n",
    "Belmont (in San Francisco Bay Area Peninsula)\n",
    "\n",
    "Ancestral homeland of the Ramaytush Ohlone\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equipment check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jupyter\n",
    "\n",
    "Run and connect to Jupyter.\n",
    "\n",
    "If you installed it locally,\n",
    "`http://localhost:8888`\n",
    "is the default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ipytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipytest\n",
    "ipytest.autoconfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There should not be any output from this step. If an error occured saying \"module not found\", make sure the virtual environment has `ipytest` installed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing and running tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FF                                                                       [100%]\n",
      "=================================== FAILURES ===================================\n",
      "______________________________ test_something[1] _______________________________\n",
      "\n",
      "value = 1\n",
      "\n",
      "    @pytest.mark.parametrize('value', [1, 2])\n",
      "    def test_something(value):\n",
      ">       assert value != value\n",
      "E       assert 1 != 1\n",
      "\n",
      "<ipython-input-2-5cd247d35d9e>:5: AssertionError\n",
      "______________________________ test_something[2] _______________________________\n",
      "\n",
      "value = 2\n",
      "\n",
      "    @pytest.mark.parametrize('value', [1, 2])\n",
      "    def test_something(value):\n",
      ">       assert value != value\n",
      "E       assert 2 != 2\n",
      "\n",
      "<ipython-input-2-5cd247d35d9e>:5: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED tmp1aczd1jx.py::test_something[1] - assert 1 != 1\n",
      "FAILED tmp1aczd1jx.py::test_something[2] - assert 2 != 2\n",
      "2 failed in 0.11s\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "import pytest\n",
    "\n",
    "@pytest.mark.parametrize('value', [1, 2])\n",
    "def test_something(value):\n",
    "    assert value != value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self-check (15:05)\n",
    "\n",
    "Run it yourself.\n",
    "\n",
    "Check to see the same thing happened!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assertions (15:10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.9.0, pytest-6.2.2, py-1.10.0, pluggy-0.13.1 -- /opt/carme/homedir/venvs/testing/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /opt/carme/homedir/src/pycon2021-tutorial-testing\n",
      "collecting ... collected 1 item\n",
      "\n",
      "tmpcz_ozh5h.py::test_something FAILED                                    [100%]\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "________________________________ test_something ________________________________\n",
      "\n",
      "    def test_something():\n",
      ">       assert 1 == 1 + 1\n",
      "E       assert 1 == 2\n",
      "E         +1\n",
      "E         -2\n",
      "\n",
      "<ipython-input-9-8b025c28988b>:2: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED tmpcz_ozh5h.py::test_something - assert 1 == 2\n",
      "============================== 1 failed in 0.02s ===============================\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -vv\n",
    "\n",
    "def test_something():\n",
    "    assert 1 == 1 + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are assertions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A test is a combination of two things:\n",
    "\n",
    "* Running the \"system under test\"\n",
    "* Checking the results\n",
    "\n",
    "Assertions help check the result is correct.\n",
    "Unless the only goal of the test is to check the SUT\n",
    "ran without errors, you will need to check something.\n",
    "\n",
    "Assertions in `pytest` use the Python `assert` statement.\n",
    "\n",
    "The statement checks that its input is a truthy value,\n",
    "and otherwise raises an `AssertionError`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AssertionError('assert 1 == 0')\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    assert 1 == 0\n",
    "except AssertionError as exc:\n",
    "    print(repr(exc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AssertionError('math is still ok\\nassert 1 == 0')\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    assert 1 == 0, \"math is still ok\"\n",
    "except AssertionError as exc:\n",
    "    print(repr(exc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    assert 1 == 1, \"math is weird\"\n",
    "except AssertionError as exc:\n",
    "    print(repr(exc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Pytest handles assertions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.9.0, pytest-6.2.2, py-1.10.0, pluggy-0.13.1 -- /opt/carme/homedir/venvs/testing/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /opt/carme/homedir/src/pycon2021-tutorial-testing\n",
      "collecting ... collected 1 item\n",
      "\n",
      "tmpqat0qwet.py::test_math FAILED                                         [100%]\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "__________________________________ test_math ___________________________________\n",
      "\n",
      "    def test_math():\n",
      ">       assert 1 == 0\n",
      "E       assert 1 == 0\n",
      "E         +1\n",
      "E         -0\n",
      "\n",
      "<ipython-input-13-66fbb4633453>:4: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED tmpqat0qwet.py::test_math - assert 1 == 0\n",
      "============================== 1 failed in 0.02s ===============================\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -vv\n",
    "\n",
    "import pytest\n",
    "\n",
    "def test_math():\n",
    "    assert 1 == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When tests are running in `pytest`,\n",
    "it modifies the `assert` statement\n",
    "so that it can give more informative errors.\n",
    "\n",
    "It will give diffs, further details,\n",
    "or explanations as appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading assertion failures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first part is the *running output*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will often see this in build log,\n",
    "or on the console,\n",
    "*while* the tests are running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shows you the status of each test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "============================= test session starts ==============================\n",
    "platform linux -- Python 3.9.0, pytest-6.1.2, py-1.9.0, pluggy-0.13.1 -- /opt/carme/venvs/testing-in-python/bin/python\n",
    "cachedir: .pytest_cache\n",
    "rootdir: /opt/carme/src/testing-in-python/session-1\n",
    "collecting ... collected 1 item\n",
    "\n",
    "tmpz_pjyxpu.py::test_something FAILED                                    [100%]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next part is the\n",
    "*failure details section*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will have a subsection for each failing test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It shows you details about the failure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Code snippet\n",
    "* Values of relevant parts of the assertion\n",
    "* (Sometimes) a diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "=================================== FAILURES ===================================\n",
    "________________________________ test_something ________________________________\n",
    "\n",
    "    def test_something():\n",
    ">       assert 1 == 1 + 1\n",
    "E       assert 1 == 2\n",
    "E         +1\n",
    "E         -2\n",
    "\n",
    "<ipython-input-3-8b025c28988b>:2: AssertionError\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last part is the failure summary. It shows you how many tests succeeded and failed,\n",
    "and a few details about each success/failure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous example was so minimal, it is not obvious how the parts relate.\n",
    "You can learn from a slightly more complicated example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.9.0, pytest-6.2.2, py-1.10.0, pluggy-0.13.1 -- /opt/carme/homedir/venvs/testing/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /opt/carme/homedir/src/pycon2021-tutorial-testing\n",
      "collecting ... collected 3 items\n",
      "\n",
      "tmpzm_7qx0l.py::test_error FAILED                                        [ 33%]\n",
      "tmpzm_7qx0l.py::test_succeed PASSED                                      [ 66%]\n",
      "tmpzm_7qx0l.py::test_fail FAILED                                         [100%]\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "__________________________________ test_error __________________________________\n",
      "\n",
      "    def test_error():\n",
      ">       assert 0 == 1\n",
      "E       assert 0 == 1\n",
      "E         +0\n",
      "E         -1\n",
      "\n",
      "<ipython-input-14-da1b2907cf4e>:2: AssertionError\n",
      "__________________________________ test_fail ___________________________________\n",
      "\n",
      "    def test_fail():\n",
      ">       assert 1/0 == 0\n",
      "E       ZeroDivisionError: division by zero\n",
      "\n",
      "<ipython-input-14-da1b2907cf4e>:8: ZeroDivisionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED tmpzm_7qx0l.py::test_error - assert 0 == 1\n",
      "FAILED tmpzm_7qx0l.py::test_fail - ZeroDivisionError: division by zero\n",
      "========================= 2 failed, 1 passed in 0.04s ==========================\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -vv\n",
    "\n",
    "def test_error():\n",
    "    assert 0 == 1\n",
    "    \n",
    "def test_succeed():\n",
    "    assert 1 == 1\n",
    "    \n",
    "def test_fail():\n",
    "    assert 1/0 == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a few things to notice:\n",
    "\n",
    "* A succeeding test is not mentioned by name in the summary, but it is counted\n",
    "* A failure that is *not* caused by an assertion is a lot less obvious."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example: you have a function that implements addition.\n",
    "Unfortunately, it has a bug -- it returns the expected result with a small difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.9.0, pytest-6.2.2, py-1.10.0, pluggy-0.13.1 -- /opt/carme/homedir/venvs/testing/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /opt/carme/homedir/src/pycon2021-tutorial-testing\n",
      "collecting ... collected 1 item\n",
      "\n",
      "tmpdgyj90me.py::test_add FAILED                                          [100%]\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "___________________________________ test_add ___________________________________\n",
      "\n",
      "    def test_add():\n",
      ">       assert add(3, 4) == 7\n",
      "E       assert 7.1 == 7\n",
      "E         +7.1\n",
      "E         -7\n",
      "\n",
      "<ipython-input-15-fb1ea62501aa>:8: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED tmpdgyj90me.py::test_add - assert 7.1 == 7\n",
      "============================== 1 failed in 0.02s ===============================\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -vv\n",
    "\n",
    "def add(a, b):\n",
    "    result = 0.1 # This is a mistake -- should be 0\n",
    "    result += a\n",
    "    result += b\n",
    "    return result\n",
    "\n",
    "def test_add():\n",
    "    assert add(3, 4) == 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example contains few lines of code, but still has some subtleties to unpack:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The test was comparing `add()` to the *expected result* because, presumably, this is what the function was documented to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The test did not care about the *implementation* of `add()`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to implement `add()` (incorrectly) is here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.9.0, pytest-6.2.2, py-1.10.0, pluggy-0.13.1 -- /opt/carme/homedir/venvs/testing/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /opt/carme/homedir/src/pycon2021-tutorial-testing\n",
      "collecting ... collected 1 item\n",
      "\n",
      "tmpx88zqy25.py::test_add FAILED                                          [100%]\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "___________________________________ test_add ___________________________________\n",
      "\n",
      "    def test_add():\n",
      ">       assert add(3, 4) == 7\n",
      "E       assert 7.1 == 7\n",
      "E         +7.1\n",
      "E         -7\n",
      "\n",
      "<ipython-input-16-a4cd36af42f3>:9: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED tmpx88zqy25.py::test_add - assert 7.1 == 7\n",
      "============================== 1 failed in 0.02s ===============================\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -vv\n",
    "\n",
    "def add(a, b):\n",
    "    return (\n",
    "        a +\n",
    "        b + \n",
    "        0.1 # This is a mistake -- should be 0\n",
    "    )\n",
    "\n",
    "def test_add():\n",
    "    assert add(3, 4) == 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test failed in precisely the same way. The test only checks `add()` against the documented guarantees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because in this case, the documented guarantees are strict and possible to plan for,\n",
    "this test works well with an equality assertion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With lists\n",
    "there are three potential things that can happen.\n",
    "\n",
    "1. The list on the right might be a proper prefix of the list on the left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.9.0, pytest-6.2.2, py-1.10.0, pluggy-0.13.1 -- /opt/carme/homedir/venvs/testing/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /opt/carme/homedir/src/pycon2021-tutorial-testing\n",
      "collecting ... collected 1 item\n",
      "\n",
      "tmpy4a8yutz.py::test_missing FAILED                                      [100%]\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "_________________________________ test_missing _________________________________\n",
      "\n",
      "    def test_missing():\n",
      ">       assert [1] == []\n",
      "E       assert [1] == []\n",
      "E         Left contains one more item: 1\n",
      "E         Full diff:\n",
      "E         - []\n",
      "E         + [1]\n",
      "E         ?  +\n",
      "\n",
      "<ipython-input-17-9f5f795e0864>:2: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED tmpy4a8yutz.py::test_missing - assert [1] == []\n",
      "============================== 1 failed in 0.02s ===============================\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -vv\n",
    "\n",
    "def test_missing():\n",
    "    assert [1] == []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The list on the left might be a proper prefix of the list on the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.9.0, pytest-6.2.2, py-1.10.0, pluggy-0.13.1 -- /opt/carme/homedir/venvs/testing/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /opt/carme/homedir/src/pycon2021-tutorial-testing\n",
      "collecting ... collected 1 item\n",
      "\n",
      "tmp3wcsi013.py::test_extra FAILED                                        [100%]\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "__________________________________ test_extra __________________________________\n",
      "\n",
      "    def test_extra():\n",
      ">       assert [1] == [1, 2]\n",
      "E       assert [1] == [1, 2]\n",
      "E         Right contains one more item: 2\n",
      "E         Full diff:\n",
      "E         - [1, 2]\n",
      "E         + [1]\n",
      "\n",
      "<ipython-input-18-0bf7a5e6c5dc>:2: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED tmp3wcsi013.py::test_extra - assert [1] == [1, 2]\n",
      "============================== 1 failed in 0.02s ===============================\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -vv\n",
    "\n",
    "def test_extra():\n",
    "    assert [1] == [1, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. At least in one index, the element at that index in both lists is different:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.9.0, pytest-6.2.2, py-1.10.0, pluggy-0.13.1 -- /opt/carme/homedir/venvs/testing/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /opt/carme/homedir/src/pycon2021-tutorial-testing\n",
      "collecting ... collected 1 item\n",
      "\n",
      "tmp0bmw7bkc.py::test_different FAILED                                    [100%]\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "________________________________ test_different ________________________________\n",
      "\n",
      "    def test_different():\n",
      ">       assert [1, 2, 3] == [1, 4, 3]\n",
      "E       assert [1, 2, 3] == [1, 4, 3]\n",
      "E         At index 1 diff: 2 != 4\n",
      "E         Full diff:\n",
      "E         - [1, 4, 3]\n",
      "E         ?     ^\n",
      "E         + [1, 2, 3]\n",
      "E         ?     ^\n",
      "\n",
      "<ipython-input-19-afb6632ba316>:2: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED tmp0bmw7bkc.py::test_different - assert [1, 2, 3] == [1, 4, 3]\n",
      "============================== 1 failed in 0.02s ===============================\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -vv\n",
    "\n",
    "def test_different():\n",
    "    assert [1, 2, 3] == [1, 4, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These failures are *different*.\n",
    "Learning how to read the failures,\n",
    "and understanding what's wrong is important.\n",
    "This is not just important when troubleshooting a failing test.\n",
    "It is also important when writing tests:\n",
    "remember, every assertion has to pay rent by being simulated\n",
    "with a bug in the system under test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is common to compare strings:\n",
    "from expected output to running external commands,\n",
    "many things are strings that have strict equality guarantees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.9.0, pytest-6.2.2, py-1.10.0, pluggy-0.13.1 -- /opt/carme/homedir/venvs/testing/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /opt/carme/homedir/src/pycon2021-tutorial-testing\n",
      "collecting ... collected 1 item\n",
      "\n",
      "tmpae0y3y2u.py::test_string FAILED                                       [100%]\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "_________________________________ test_string __________________________________\n",
      "\n",
      "    def test_string():\n",
      ">       assert \"hello\\nworld\" == \"goodbye\\nworld\"\n",
      "E       AssertionError: assert 'hello\\nworld' == 'goodbye\\nworld'\n",
      "E         - goodbye\n",
      "E         + hello\n",
      "E           world\n",
      "\n",
      "<ipython-input-20-53db1bf85550>:2: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED tmpae0y3y2u.py::test_string - AssertionError: assert 'hello\\nworld' ==...\n",
      "============================== 1 failed in 0.02s ===============================\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -vv\n",
    "\n",
    "def test_string():\n",
    "    assert \"hello\\nworld\" == \"goodbye\\nworld\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the length of the string, sometimes an inside-the-line diff will be triggered:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.9.0, pytest-6.2.2, py-1.10.0, pluggy-0.13.1 -- /opt/carme/homedir/venvs/testing/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /opt/carme/homedir/src/pycon2021-tutorial-testing\n",
      "collecting ... collected 1 item\n",
      "\n",
      "tmp0zt_5y64.py::test_string FAILED                                       [100%]\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "_________________________________ test_string __________________________________\n",
      "\n",
      "    def test_string():\n",
      ">       assert \"saying \" * 10 + \"hello world\" + \" said\" * 10 == \"saying \" * 10 + \"goodbye world\" + \" said\" * 10\n",
      "E       AssertionError: assert 'saying sayin...aid said said' == 'saying sayin...aid said said'\n",
      "E         - saying saying saying saying saying saying saying saying saying saying goodbye world said said said said said said said said said said\n",
      "E         ?                                                                       ^ -----\n",
      "E         + saying saying saying saying saying saying saying saying saying saying hello world said said said said said said said said said said\n",
      "E         ?                                                                       ^^^^\n",
      "\n",
      "<ipython-input-21-d0bd7a1cc63c>:2: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED tmp0zt_5y64.py::test_string - AssertionError: assert 'saying sayin...a...\n",
      "============================== 1 failed in 0.03s ===============================\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -vv\n",
    "\n",
    "def test_string():\n",
    "    assert \"saying \" * 10 + \"hello world\" + \" said\" * 10 == \"saying \" * 10 + \"goodbye world\" + \" said\" * 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pytest` will try to give useful diffs with most containers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.9.0, pytest-6.2.2, py-1.10.0, pluggy-0.13.1 -- /opt/carme/homedir/venvs/testing/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /opt/carme/homedir/src/pycon2021-tutorial-testing\n",
      "collecting ... collected 1 item\n",
      "\n",
      "tmp4hghrvi7.py::test_set FAILED                                          [100%]\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "___________________________________ test_set ___________________________________\n",
      "\n",
      "    def test_set():\n",
      ">       assert set([1,2]) == set([1,3])\n",
      "E       AssertionError: assert {1, 2} == {1, 3}\n",
      "E         Extra items in the left set:\n",
      "E         2\n",
      "E         Extra items in the right set:\n",
      "E         3\n",
      "E         Full diff:\n",
      "E         - {1, 3}\n",
      "E         ?     ^...\n",
      "E         \n",
      "E         ...Full output truncated (3 lines hidden), use '-vv' to show\n",
      "\n",
      "<ipython-input-22-62b056d135fa>:2: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED tmp4hghrvi7.py::test_set - AssertionError: assert {1, 2} == {1, 3}\n",
      "============================== 1 failed in 0.03s ===============================\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -vv\n",
    "\n",
    "def test_set():\n",
    "    assert set([1,2]) == set([1,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For sets, it will check for spurious elements on both sides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tip -- Inserting Exceptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inserting exceptions into tests is a surprisingly good way to debug test failures.\n",
    "\n",
    "* You can make sure the test fails: test fail with exceptions!\n",
    "* The exception -> test failure output is the most reliable output path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.9.0, pytest-6.2.2, py-1.10.0, pluggy-0.13.1 -- /opt/carme/homedir/venvs/testing/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /opt/carme/homedir/src/pycon2021-tutorial-testing\n",
      "collecting ... collected 2 items\n",
      "\n",
      "tmp15b963xn.py::test_subtle_manipulation FAILED                          [ 50%]\n",
      "tmp15b963xn.py::test_subtle_manipulation_2 FAILED                        [100%]\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "___________________________ test_subtle_manipulation ___________________________\n",
      "\n",
      "    def test_subtle_manipulation():\n",
      ">       assert subtle_manipulation(\"--HELLO--\") == \"HELLO\"\n",
      "E       AssertionError: assert '-HELLO-' == 'HELLO'\n",
      "E         - HELLO\n",
      "E         + -HELLO-\n",
      "E         ? +     +\n",
      "\n",
      "<ipython-input-23-a523be4640ec>:6: AssertionError\n",
      "__________________________ test_subtle_manipulation_2 __________________________\n",
      "\n",
      "    def test_subtle_manipulation_2():\n",
      ">       assert erroring_subtle_manipulation(\"--HELLO--\") == \"HELLO\"\n",
      "\n",
      "<ipython-input-23-a523be4640ec>:15: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "things = '--HELLO--'\n",
      "\n",
      "    def erroring_subtle_manipulation(things):\n",
      "        trim = 1\n",
      "        ret_value = things[trim:-trim]\n",
      ">       raise ValueError(things, ret_value, trim)\n",
      "E       ValueError: ('--HELLO--', '-HELLO-', 1)\n",
      "\n",
      "<ipython-input-23-a523be4640ec>:11: ValueError\n",
      "=========================== short test summary info ============================\n",
      "FAILED tmp15b963xn.py::test_subtle_manipulation - AssertionError: assert '-HE...\n",
      "FAILED tmp15b963xn.py::test_subtle_manipulation_2 - ValueError: ('--HELLO--',...\n",
      "============================== 2 failed in 0.04s ===============================\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -vv\n",
    "\n",
    "def subtle_manipulation(things):\n",
    "    trim = 1 # It should be 2\n",
    "    return things[trim:-trim]\n",
    "\n",
    "def test_subtle_manipulation():\n",
    "    assert subtle_manipulation(\"--HELLO--\") == \"HELLO\"\n",
    "\n",
    "def erroring_subtle_manipulation(things):\n",
    "    trim = 1\n",
    "    ret_value = things[trim:-trim]\n",
    "    raise ValueError(things, ret_value, trim)\n",
    "    return ret\n",
    "\n",
    "def test_subtle_manipulation_2():\n",
    "    assert erroring_subtle_manipulation(\"--HELLO--\") == \"HELLO\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other assertions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually inequality is asserted as a \"helper\" assertion.\n",
    "For example, if a function is defined to produce a \"different\" example for something,\n",
    "it might be reasonable to also verify that these examples are all indeed different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytest will still print the values,\n",
    "but in this case, there is no diff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.9.0, pytest-6.2.2, py-1.10.0, pluggy-0.13.1 -- /opt/carme/homedir/venvs/testing/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /opt/carme/homedir/src/pycon2021-tutorial-testing\n",
      "collecting ... collected 1 item\n",
      "\n",
      "tmpanc247e4.py::test_not_equal FAILED                                    [100%]\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "________________________________ test_not_equal ________________________________\n",
      "\n",
      "    def test_not_equal():\n",
      "        x = 1\n",
      "        y = x / 1\n",
      ">       assert x != y\n",
      "E       assert 1 != 1.0\n",
      "\n",
      "<ipython-input-24-2c77592b27e5>:4: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED tmpanc247e4.py::test_not_equal - assert 1 != 1.0\n",
      "============================== 1 failed in 0.02s ===============================\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -vv\n",
    "\n",
    "def test_not_equal():\n",
    "    x = 1\n",
    "    y = x / 1\n",
    "    assert x != y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inequality is not just for numbers:\n",
    "there are other objects that might need to be compared for inequality.\n",
    "Again, notice what pytest does and does not output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.9.0, pytest-6.2.2, py-1.10.0, pluggy-0.13.1 -- /opt/carme/homedir/venvs/testing/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /opt/carme/homedir/src/pycon2021-tutorial-testing\n",
      "collecting ... collected 1 item\n",
      "\n",
      "tmpo69af7vh.py::test_not_equal_lists FAILED                              [100%]\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "_____________________________ test_not_equal_lists _____________________________\n",
      "\n",
      "    def test_not_equal_lists():\n",
      "        x = [1, 2, 3]\n",
      ">       assert x != x[:]\n",
      "E       assert [1, 2, 3] != [1, 2, 3]\n",
      "\n",
      "<ipython-input-25-166f9e15d717>:3: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED tmpo69af7vh.py::test_not_equal_lists - assert [1, 2, 3] != [1, 2, 3]\n",
      "============================== 1 failed in 0.02s ===============================\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -vv\n",
    "\n",
    "def test_not_equal_lists():\n",
    "    x = [1, 2, 3]\n",
    "    assert x != x[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A stronger assertion is *order*.\n",
    "Both the `<` and `<=` operators, as well as their inverses, can be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.9.0, pytest-6.2.2, py-1.10.0, pluggy-0.13.1 -- /opt/carme/homedir/venvs/testing/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /opt/carme/homedir/src/pycon2021-tutorial-testing\n",
      "collecting ... collected 1 item\n",
      "\n",
      "tmpkif5pcri.py::test_greater FAILED                                      [100%]\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "_________________________________ test_greater _________________________________\n",
      "\n",
      "    def test_greater():\n",
      "        x = 1\n",
      ">       assert x < x\n",
      "E       assert 1 < 1\n",
      "\n",
      "<ipython-input-26-376211f62504>:3: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED tmpkif5pcri.py::test_greater - assert 1 < 1\n",
      "============================== 1 failed in 0.02s ===============================\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -vv\n",
    "\n",
    "def test_greater():\n",
    "    x = 1\n",
    "    assert x < x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be mindful of which ordering operator you want:\n",
    "always think about the semantics and whether `<` or `<=` is appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.9.0, pytest-6.2.2, py-1.10.0, pluggy-0.13.1 -- /opt/carme/homedir/venvs/testing/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /opt/carme/homedir/src/pycon2021-tutorial-testing\n",
      "collecting ... collected 1 item\n",
      "\n",
      "tmppyxq0t3c.py::test_greater_or_equal FAILED                             [100%]\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "____________________________ test_greater_or_equal _____________________________\n",
      "\n",
      "    def test_greater_or_equal():\n",
      "        x = 1\n",
      ">       assert x + 1 <= x\n",
      "E       assert (1 + 1) <= 1\n",
      "\n",
      "<ipython-input-27-303e5dedf2ff>:3: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED tmppyxq0t3c.py::test_greater_or_equal - assert (1 + 1) <= 1\n",
      "============================== 1 failed in 0.02s ===============================\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -vv\n",
    "\n",
    "def test_greater_or_equal():\n",
    "    x = 1\n",
    "    assert x + 1 <= x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sets can also be compared.\n",
    "Note that there is no diff.\n",
    "Compare two tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.9.0, pytest-6.2.2, py-1.10.0, pluggy-0.13.1 -- /opt/carme/homedir/venvs/testing/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /opt/carme/homedir/src/pycon2021-tutorial-testing\n",
      "collecting ... collected 2 items\n",
      "\n",
      "tmpqi76va79.py::test_set_comparison FAILED                               [ 50%]\n",
      "tmpqi76va79.py::test_set_comparison_with_equality FAILED                 [100%]\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "_____________________________ test_set_comparison ______________________________\n",
      "\n",
      "    def test_set_comparison():\n",
      ">       assert SMALLER <= BIGGER\n",
      "E       assert {1, 2, 3} <= {1, 2}\n",
      "\n",
      "<ipython-input-28-78d2604b185f>:5: AssertionError\n",
      "______________________ test_set_comparison_with_equality _______________________\n",
      "\n",
      "    def test_set_comparison_with_equality():\n",
      ">       assert SMALLER & BIGGER == SMALLER\n",
      "E       assert {1, 2} == {1, 2, 3}\n",
      "E         Extra items in the right set:\n",
      "E         3\n",
      "E         Full diff:\n",
      "E         - {1, 2, 3}\n",
      "E         ?      ---\n",
      "E         + {1, 2}\n",
      "\n",
      "<ipython-input-28-78d2604b185f>:8: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED tmpqi76va79.py::test_set_comparison - assert {1, 2, 3} <= {1, 2}\n",
      "FAILED tmpqi76va79.py::test_set_comparison_with_equality - assert {1, 2} == {...\n",
      "============================== 2 failed in 0.04s ===============================\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -vv\n",
    "\n",
    "SMALLER = {1, 2, 3}\n",
    "BIGGER = {1, 2}\n",
    "\n",
    "def test_set_comparison():\n",
    "    assert SMALLER <= BIGGER\n",
    "    \n",
    "def test_set_comparison_with_equality():\n",
    "    assert SMALLER & BIGGER == SMALLER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `in` and `not in` operators are sometimes useful to assert about items and containers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.9.0, pytest-6.2.2, py-1.10.0, pluggy-0.13.1 -- /opt/carme/homedir/venvs/testing/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /opt/carme/homedir/src/pycon2021-tutorial-testing\n",
      "collecting ... collected 2 items\n",
      "\n",
      "tmp4x9sxyk2.py::test_in FAILED                                           [ 50%]\n",
      "tmp4x9sxyk2.py::test_not_in FAILED                                       [100%]\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "___________________________________ test_in ____________________________________\n",
      "\n",
      "    def test_in():\n",
      ">       assert 5 in range(3)\n",
      "E       assert 5 in range(0, 3)\n",
      "E        +  where range(0, 3) = range(3)\n",
      "\n",
      "<ipython-input-29-aac186ccb029>:2: AssertionError\n",
      "_________________________________ test_not_in __________________________________\n",
      "\n",
      "    def test_not_in():\n",
      ">       assert 1 not in set([1])\n",
      "E       assert 1 not in {1}\n",
      "E        +  where {1} = set([1])\n",
      "\n",
      "<ipython-input-29-aac186ccb029>:5: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED tmp4x9sxyk2.py::test_in - assert 5 in range(0, 3)\n",
      "FAILED tmp4x9sxyk2.py::test_not_in - assert 1 not in {1}\n",
      "============================== 2 failed in 0.03s ===============================\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -vv\n",
    "\n",
    "def test_in():\n",
    "    assert 5 in range(3)\n",
    "    \n",
    "def test_not_in():\n",
    "    assert 1 not in set([1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, you want to verify the actual identity of objects.\n",
    "Note that the behavior of `is` on built-in constants is awkwardly defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.9.0, pytest-6.2.2, py-1.10.0, pluggy-0.13.1 -- /opt/carme/homedir/venvs/testing/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /opt/carme/homedir/src/pycon2021-tutorial-testing\n",
      "collecting ... collected 2 items\n",
      "\n",
      "tmpm9uijcks.py::test_identical FAILED                                    [ 50%]\n",
      "tmpm9uijcks.py::test_not_identical FAILED                                [100%]\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "________________________________ test_identical ________________________________\n",
      "\n",
      "    def test_identical():\n",
      ">       assert [1, 2, 3] is [1, 2, 3]\n",
      "E       assert [1, 2, 3] is [1, 2, 3]\n",
      "\n",
      "<ipython-input-30-ab51e76b4d59>:2: AssertionError\n",
      "______________________________ test_not_identical ______________________________\n",
      "\n",
      "    def test_not_identical():\n",
      "        x = \"hello\"\n",
      ">       assert x is not x\n",
      "E       AssertionError: assert 'hello' is not 'hello'\n",
      "\n",
      "<ipython-input-30-ab51e76b4d59>:6: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED tmpm9uijcks.py::test_identical - assert [1, 2, 3] is [1, 2, 3]\n",
      "FAILED tmpm9uijcks.py::test_not_identical - AssertionError: assert 'hello' is...\n",
      "============================== 2 failed in 0.04s ===============================\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -vv\n",
    "\n",
    "def test_identical():\n",
    "    assert [1, 2, 3] is [1, 2, 3]\n",
    "    \n",
    "def test_not_identical():\n",
    "    x = \"hello\"\n",
    "    assert x is not x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All comparison operators in Python (`==` and the others)\n",
    "*chain*.\n",
    "Pytest special-cases comparison chains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.9.0, pytest-6.2.2, py-1.10.0, pluggy-0.13.1 -- /opt/carme/homedir/venvs/testing/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /opt/carme/homedir/src/pycon2021-tutorial-testing\n",
      "collecting ... collected 1 item\n",
      "\n",
      "tmp_q2_f0dl.py::test_equality_chain FAILED                               [100%]\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "_____________________________ test_equality_chain ______________________________\n",
      "\n",
      "    def test_equality_chain():\n",
      "        x = [1, 2, 3]\n",
      ">       assert [1, 2, 3] == x == [1, 2, 3, 4]\n",
      "E       assert [1, 2, 3] == [1, 2, 3, 4]\n",
      "E         Right contains one more item: 4\n",
      "E         Full diff:\n",
      "E         - [1, 2, 3, 4]\n",
      "E         ?         ---\n",
      "E         + [1, 2, 3]\n",
      "\n",
      "<ipython-input-31-fe00f4d7a82e>:3: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED tmp_q2_f0dl.py::test_equality_chain - assert [1, 2, 3] == [1, 2, 3, 4]\n",
      "============================== 1 failed in 0.02s ===============================\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -vv\n",
    "\n",
    "def test_equality_chain():\n",
    "    x = [1, 2, 3]\n",
    "    assert [1, 2, 3] == x == [1, 2, 3, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes there will be a direct boolean you want to assert.\n",
    "In that case, it is useful to put the place this boolean came from\n",
    "in the test assertion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.9.0, pytest-6.2.2, py-1.10.0, pluggy-0.13.1 -- /opt/carme/homedir/venvs/testing/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /opt/carme/homedir/src/pycon2021-tutorial-testing\n",
      "collecting ... collected 1 item\n",
      "\n",
      "tmpq5mj1chv.py::test_false FAILED                                        [100%]\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "__________________________________ test_false __________________________________\n",
      "\n",
      "    def test_false():\n",
      "        x = [1, 2, 3]\n",
      "        x.append(4)\n",
      "        x.append(4)\n",
      ">       assert always_false(x)\n",
      "E       assert False\n",
      "E        +  where False = always_false([1, 2, 3, 4, 4])\n",
      "\n",
      "<ipython-input-32-9ddcb644572d>:8: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED tmpq5mj1chv.py::test_false - assert False\n",
      "============================== 1 failed in 0.02s ===============================\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -vv\n",
    "\n",
    "def always_false(something):\n",
    "    return False\n",
    "\n",
    "def test_false():\n",
    "    x = [1, 2, 3]\n",
    "    x.append(4)\n",
    "    x.append(4)\n",
    "    assert always_false(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise (15:40-15:50)\n",
    "\n",
    "Video will pause for 10m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_pytest[clean] -vv\n",
    "\n",
    "def safe_remove(a, b):\n",
    "    pass # fix this line\n",
    "\n",
    "def test_safe_remove_no():\n",
    "    things = {1: \"yes\", 2: \"no\"}\n",
    "    safe_remove(things, 3)\n",
    "    assert 1 in things\n",
    "\n",
    "def test_safe_remove_yes():\n",
    "    things = {1: \"yes\", 2: \"no\"}\n",
    "    safe_remove(things, 2)\n",
    "    assert 2 not in things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_pytest[clean] -vv\n",
    "\n",
    "def get_min_max(a, b):\n",
    "    return a, b # fix this line\n",
    "\n",
    "def test_min_max_high():\n",
    "    a, b = get_min_max(2, 1)\n",
    "    assert set([a, b]) == set([1, 2])\n",
    "    assert a < b\n",
    "\n",
    "def test_min_max_low():\n",
    "    a, b = get_min_max(1, 2)\n",
    "    assert set([a, b]) == set([1, 2])\n",
    "    assert a < b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.9.0, pytest-6.2.2, py-1.10.0, pluggy-0.13.1 -- /opt/carme/homedir/venvs/testing/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /opt/carme/homedir/src/pycon2021-tutorial-testing\n",
      "collecting ... collected 2 items\n",
      "\n",
      "tmpj3ki0yw1.py::test_safe_remove_no PASSED                               [ 50%]\n",
      "tmpj3ki0yw1.py::test_safe_remove_yes PASSED                              [100%]\n",
      "\n",
      "============================== 2 passed in 0.03s ===============================\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -vv\n",
    "\n",
    "def safe_remove(a, b):\n",
    "    a.pop(b, None)\n",
    "\n",
    "def test_safe_remove_no():\n",
    "    things = {1: \"yes\", 2: \"no\"}\n",
    "    safe_remove(things, 3)\n",
    "    assert 1 in things\n",
    "\n",
    "def test_safe_remove_yes():\n",
    "    things = {1: \"yes\", 2: \"no\"}\n",
    "    safe_remove(things, 2)\n",
    "    assert 2 not in things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.9.0, pytest-6.2.2, py-1.10.0, pluggy-0.13.1 -- /opt/carme/homedir/venvs/testing/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /opt/carme/homedir/src/pycon2021-tutorial-testing\n",
      "collecting ... collected 2 items\n",
      "\n",
      "tmpzc6v2e7_.py::test_min_max_high PASSED                                 [ 50%]\n",
      "tmpzc6v2e7_.py::test_min_max_low PASSED                                  [100%]\n",
      "\n",
      "============================== 2 passed in 0.02s ===============================\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -vv\n",
    "\n",
    "def get_min_max(a, b):\n",
    "    return min([a, b]), max([a, b])\n",
    "\n",
    "def test_min_max_high():\n",
    "    a, b = get_min_max(2, 1)\n",
    "    assert set([a, b]) == set([1, 2])\n",
    "    assert a < b\n",
    "\n",
    "def test_min_max_low():\n",
    "    a, b = get_min_max(1, 2)\n",
    "    assert set([a, b]) == set([1, 2])\n",
    "    assert a < b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "* Tests: run + verify\n",
    "* Assertions verify\n",
    "* Pytest rewrites `assert`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Break (16:00 - 16:10)\n",
    "\n",
    "Video will pause for 10m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mock basics (16:10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are mocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mocks are found in the module `unittest.mock` in the standard library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common class is `MagicMock()`, and will be the main one this class covers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to see mocks in action, you can intentionally fail some tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing a function that raises an exception is a good way to see how mocks work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.9.0, pytest-6.2.2, py-1.10.0, pluggy-0.13.1 -- /opt/carme/homedir/venvs/testing/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /opt/carme/homedir/src/pycon2021-tutorial-testing\n",
      "collecting ... collected 1 item\n",
      "\n",
      "tmpcjh3ewal.py::test_value FAILED                                        [100%]\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "__________________________________ test_value __________________________________\n",
      "\n",
      "    def test_value():\n",
      ">       raise_value(mock.MagicMock())\n",
      "\n",
      "<ipython-input-10-f5a55793a001>:7: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "x = <MagicMock id='140677003758944'>\n",
      "\n",
      "    def raise_value(x):\n",
      ">       raise ValueError(x)\n",
      "E       ValueError: <MagicMock id='140677003758944'>\n",
      "\n",
      "<ipython-input-10-f5a55793a001>:4: ValueError\n",
      "=========================== short test summary info ============================\n",
      "FAILED tmpcjh3ewal.py::test_value - ValueError: <MagicMock id='140677003758944'>\n",
      "============================== 1 failed in 0.03s ===============================\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -vv\n",
    "\n",
    "from unittest import mock\n",
    "\n",
    "def raise_value(x):\n",
    "    raise ValueError(x)\n",
    "\n",
    "def test_value():\n",
    "    raise_value(mock.MagicMock())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default mock properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mocks have every attribute. It is also a mock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.9.0, pytest-6.2.2, py-1.10.0, pluggy-0.13.1 -- /opt/carme/homedir/venvs/testing/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /opt/carme/homedir/src/pycon2021-tutorial-testing\n",
      "collecting ... collected 1 item\n",
      "\n",
      "tmpsnd26nvx.py::test_name FAILED                                         [100%]\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "__________________________________ test_name ___________________________________\n",
      "\n",
      "    def test_name():\n",
      ">       raise_some_name(mock.MagicMock())\n",
      "\n",
      "<ipython-input-11-d228cf1cd83d>:7: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "x = <MagicMock id='140677003089904'>\n",
      "\n",
      "    def raise_some_name(x):\n",
      ">       raise ValueError(x.some_name)\n",
      "E       ValueError: <MagicMock name='mock.some_name' id='140677002383760'>\n",
      "\n",
      "<ipython-input-11-d228cf1cd83d>:4: ValueError\n",
      "=========================== short test summary info ============================\n",
      "FAILED tmpsnd26nvx.py::test_name - ValueError: <MagicMock name='mock.some_nam...\n",
      "============================== 1 failed in 0.03s ===============================\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -vv\n",
    "\n",
    "from unittest import mock\n",
    "\n",
    "def raise_some_name(x):\n",
    "    raise ValueError(x.some_name)\n",
    "\n",
    "def test_name():\n",
    "    raise_some_name(mock.MagicMock())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A mock's attribtues are *consistent*.\n",
    "They stay the same, so retrieving the same attribute again gives identical objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.9.0, pytest-6.2.2, py-1.10.0, pluggy-0.13.1 -- /opt/carme/homedir/venvs/testing/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /opt/carme/homedir/src/pycon2021-tutorial-testing\n",
      "collecting ... collected 1 item\n",
      "\n",
      "tmpewqt3wqb.py::test_consistent FAILED                                   [100%]\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "_______________________________ test_consistent ________________________________\n",
      "\n",
      "    def test_consistent():\n",
      "        obj = mock.MagicMock()\n",
      ">       assert obj.some_name is not obj.some_name\n",
      "E       AssertionError: assert <MagicMock name='mock.some_name' id='140677002593760'> is not <MagicMock name='mock.some_name' id='140677002593760'>\n",
      "E        +  where <MagicMock name='mock.some_name' id='140677002593760'> = <MagicMock id='140677003671968'>.some_name\n",
      "E        +  and   <MagicMock name='mock.some_name' id='140677002593760'> = <MagicMock id='140677003671968'>.some_name\n",
      "\n",
      "<ipython-input-12-f8979df88299>:5: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED tmpewqt3wqb.py::test_consistent - AssertionError: assert <MagicMock na...\n",
      "============================== 1 failed in 0.02s ===============================\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -vv\n",
    "\n",
    "from unittest import mock\n",
    "\n",
    "def test_consistent():\n",
    "    obj = mock.MagicMock()\n",
    "    assert obj.some_name is not obj.some_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calling mocks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mocks can be called. They return a mock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.9.0, pytest-6.2.2, py-1.10.0, pluggy-0.13.1 -- /opt/carme/homedir/venvs/testing/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /opt/carme/homedir/src/pycon2021-tutorial-testing\n",
      "collecting ... collected 1 item\n",
      "\n",
      "tmp61wla3mm.py::test_examine FAILED                                      [100%]\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "_________________________________ test_examine _________________________________\n",
      "\n",
      "    def test_examine():\n",
      ">       raise_call(mock.MagicMock())\n",
      "\n",
      "<ipython-input-13-df7c5f12fdad>:7: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "x = <MagicMock id='140677001869152'>\n",
      "\n",
      "    def raise_call(x):\n",
      ">       raise ValueError(x())\n",
      "E       ValueError: <MagicMock name='mock()' id='140677001881104'>\n",
      "\n",
      "<ipython-input-13-df7c5f12fdad>:4: ValueError\n",
      "=========================== short test summary info ============================\n",
      "FAILED tmp61wla3mm.py::test_examine - ValueError: <MagicMock name='mock()' id...\n",
      "============================== 1 failed in 0.03s ===============================\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -vv\n",
    "\n",
    "from unittest import mock\n",
    "\n",
    "def raise_call(x):\n",
    "    raise ValueError(x())\n",
    "\n",
    "def test_examine():\n",
    "    raise_call(mock.MagicMock())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When calling a mock again. it will again return the same value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.9.0, pytest-6.2.2, py-1.10.0, pluggy-0.13.1 -- /opt/carme/homedir/venvs/testing/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /opt/carme/homedir/src/pycon2021-tutorial-testing\n",
      "collecting ... collected 1 item\n",
      "\n",
      "tmp_o01keh4.py::test_consistent_call FAILED                              [100%]\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "_____________________________ test_consistent_call _____________________________\n",
      "\n",
      "    def test_consistent_call():\n",
      "        obj = mock.MagicMock()\n",
      ">       assert obj() is not obj()\n",
      "E       AssertionError: assert <MagicMock name='mock()' id='140677001667920'> is not <MagicMock name='mock()' id='140677001667920'>\n",
      "E        +  where <MagicMock name='mock()' id='140677001667920'> = <MagicMock id='140677002662144'>()\n",
      "E        +  and   <MagicMock name='mock()' id='140677001667920'> = <MagicMock id='140677002662144'>()\n",
      "\n",
      "<ipython-input-14-dcd2cf06f1ca>:5: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED tmp_o01keh4.py::test_consistent_call - AssertionError: assert <MagicMo...\n",
      "============================== 1 failed in 0.03s ===============================\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -vv\n",
    "\n",
    "from unittest import mock\n",
    "\n",
    "def test_consistent_call():\n",
    "    obj = mock.MagicMock()\n",
    "    assert obj() is not obj()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because an attribute returns a mock, and mocks can be called,\n",
    "mocks also have all the methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.9.0, pytest-6.2.2, py-1.10.0, pluggy-0.13.1 -- /opt/carme/homedir/venvs/testing/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /opt/carme/homedir/src/pycon2021-tutorial-testing\n",
      "collecting ... collected 1 item\n",
      "\n",
      "tmp32xp4w39.py::test_deep FAILED                                         [100%]\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "__________________________________ test_deep ___________________________________\n",
      "\n",
      "    def test_deep():\n",
      ">       raise_deep(mock.MagicMock())\n",
      "\n",
      "<ipython-input-15-b3b5241656e2>:8: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "x = <MagicMock id='140677002209552'>\n",
      "\n",
      "    def raise_deep(x):\n",
      "        val = x.some_method()\n",
      ">       raise ValueError(val.some_attribute)\n",
      "E       ValueError: <MagicMock name='mock.some_method().some_attribute' id='140677001389728'>\n",
      "\n",
      "<ipython-input-15-b3b5241656e2>:5: ValueError\n",
      "=========================== short test summary info ============================\n",
      "FAILED tmp32xp4w39.py::test_deep - ValueError: <MagicMock name='mock.some_met...\n",
      "============================== 1 failed in 0.03s ===============================\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -vv\n",
    "\n",
    "from unittest import mock\n",
    "\n",
    "def raise_deep(x):\n",
    "    val = x.some_method()\n",
    "    raise ValueError(val.some_attribute)\n",
    "    \n",
    "def test_deep():\n",
    "    raise_deep(mock.MagicMock())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mock magic methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Magic` in `MagicMock` is because it also has the so-called \"magic methods\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those are the methods that allow objects to overload operations like addition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means mocks can also be added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.9.0, pytest-6.2.2, py-1.10.0, pluggy-0.13.1 -- /opt/carme/homedir/venvs/testing/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /opt/carme/homedir/src/pycon2021-tutorial-testing\n",
      "collecting ... collected 1 item\n",
      "\n",
      "tmpgd6n2l1i.py::test_add_1 FAILED                                        [100%]\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "__________________________________ test_add_1 __________________________________\n",
      "\n",
      "    def test_add_1():\n",
      ">       raise_add_1(mock.MagicMock())\n",
      "\n",
      "<ipython-input-16-e43521261d4b>:8: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "x = <MagicMock id='140677001236384'>\n",
      "\n",
      "    def raise_add_1(x):\n",
      "        val = x + 1\n",
      ">       raise ValueError(val)\n",
      "E       ValueError: <MagicMock name='mock.__add__()' id='140677000847568'>\n",
      "\n",
      "<ipython-input-16-e43521261d4b>:5: ValueError\n",
      "=========================== short test summary info ============================\n",
      "FAILED tmpgd6n2l1i.py::test_add_1 - ValueError: <MagicMock name='mock.__add__...\n",
      "============================== 1 failed in 0.03s ===============================\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -vv\n",
    "\n",
    "from unittest import mock\n",
    "\n",
    "def raise_add_1(x):\n",
    "    val = x + 1\n",
    "    raise ValueError(val)\n",
    "    \n",
    "def test_add_1():\n",
    "    raise_add_1(mock.MagicMock())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because calling methods on a mock returns the same value, it does not matter what we add."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.9.0, pytest-6.2.2, py-1.10.0, pluggy-0.13.1 -- /opt/carme/homedir/venvs/testing/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /opt/carme/homedir/src/pycon2021-tutorial-testing\n",
      "collecting ... collected 1 item\n",
      "\n",
      "tmpvm6wfbjl.py::test_add_different FAILED                                [100%]\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "______________________________ test_add_different ______________________________\n",
      "\n",
      "    def test_add_different():\n",
      "        x = mock.MagicMock()\n",
      ">       assert x + 1 != x + 5\n",
      "E       AssertionError: assert (<MagicMock id='140677000624448'> + 1) != (<MagicMock id='140677000624448'> + 5)\n",
      "\n",
      "<ipython-input-17-967222547642>:5: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED tmpvm6wfbjl.py::test_add_different - AssertionError: assert (<MagicMoc...\n",
      "============================== 1 failed in 0.02s ===============================\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -vv\n",
    "\n",
    "from unittest import mock\n",
    "    \n",
    "def test_add_different():\n",
    "    x = mock.MagicMock()\n",
    "    assert x + 1 != x + 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterating over values is possible, but mocks don't yield any elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.9.0, pytest-6.2.2, py-1.10.0, pluggy-0.13.1 -- /opt/carme/homedir/venvs/testing/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /opt/carme/homedir/src/pycon2021-tutorial-testing\n",
      "collecting ... collected 1 item\n",
      "\n",
      "tmp8xnls4vy.py::test_iterate FAILED                                      [100%]\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "_________________________________ test_iterate _________________________________\n",
      "\n",
      "    def test_iterate():\n",
      ">       iterate_over(mock.MagicMock())\n",
      "\n",
      "<ipython-input-18-b327e7566ccc>:9: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "x = <MagicMock id='140677000992944'>\n",
      "\n",
      "    def iterate_over(x):\n",
      "        for el in x:\n",
      "            raise ValueError(el)\n",
      ">       raise ValueError(\"no elements\", x)\n",
      "E       ValueError: ('no elements', <MagicMock id='140677000992944'>)\n",
      "\n",
      "<ipython-input-18-b327e7566ccc>:6: ValueError\n",
      "=========================== short test summary info ============================\n",
      "FAILED tmp8xnls4vy.py::test_iterate - ValueError: ('no elements', <MagicMock ...\n",
      "============================== 1 failed in 0.03s ===============================\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -vv\n",
    "\n",
    "from unittest import mock\n",
    "\n",
    "def iterate_over(x):\n",
    "    for el in x:\n",
    "        raise ValueError(el)\n",
    "    raise ValueError(\"no elements\", x)\n",
    "    \n",
    "def test_iterate():\n",
    "    iterate_over(mock.MagicMock())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardless whether you use the `[]` operator with indices, slices, or step-slices, mocks will return the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.9.0, pytest-6.2.2, py-1.10.0, pluggy-0.13.1 -- /opt/carme/homedir/venvs/testing/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /opt/carme/homedir/src/pycon2021-tutorial-testing\n",
      "collecting ... collected 3 items\n",
      "\n",
      "tmp2jii14m2.py::test_index FAILED                                        [ 33%]\n",
      "tmp2jii14m2.py::test_slice FAILED                                        [ 66%]\n",
      "tmp2jii14m2.py::test_step_slice FAILED                                   [100%]\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "__________________________________ test_index __________________________________\n",
      "\n",
      "    def test_index():\n",
      ">       raise_index(mock.MagicMock())\n",
      "\n",
      "<ipython-input-19-8c6b9a3dd63a>:13: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "x = <MagicMock id='140676999719904'>\n",
      "\n",
      "    def raise_index(x):\n",
      ">       raise ValueError(x[5])\n",
      "E       ValueError: <MagicMock name='mock.__getitem__()' id='140676999786160'>\n",
      "\n",
      "<ipython-input-19-8c6b9a3dd63a>:4: ValueError\n",
      "__________________________________ test_slice __________________________________\n",
      "\n",
      "    def test_slice():\n",
      ">       raise_slice(mock.MagicMock())\n",
      "\n",
      "<ipython-input-19-8c6b9a3dd63a>:16: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "x = <MagicMock id='140677000685504'>\n",
      "\n",
      "    def raise_slice(x):\n",
      ">       raise ValueError(x[5:7])\n",
      "E       ValueError: <MagicMock name='mock.__getitem__()' id='140677000993472'>\n",
      "\n",
      "<ipython-input-19-8c6b9a3dd63a>:7: ValueError\n",
      "_______________________________ test_step_slice ________________________________\n",
      "\n",
      "    def test_step_slice():\n",
      ">       raise_step_slice(mock.MagicMock())\n",
      "\n",
      "<ipython-input-19-8c6b9a3dd63a>:19: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "x = <MagicMock id='140677000064688'>\n",
      "\n",
      "    def raise_step_slice(x):\n",
      ">       raise ValueError(x[::-1])\n",
      "E       ValueError: <MagicMock name='mock.__getitem__()' id='140676999813104'>\n",
      "\n",
      "<ipython-input-19-8c6b9a3dd63a>:10: ValueError\n",
      "=========================== short test summary info ============================\n",
      "FAILED tmp2jii14m2.py::test_index - ValueError: <MagicMock name='mock.__getit...\n",
      "FAILED tmp2jii14m2.py::test_slice - ValueError: <MagicMock name='mock.__getit...\n",
      "FAILED tmp2jii14m2.py::test_step_slice - ValueError: <MagicMock name='mock.__...\n",
      "============================== 3 failed in 0.07s ===============================\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -vv\n",
    "\n",
    "from unittest import mock\n",
    "\n",
    "def raise_index(x):\n",
    "    raise ValueError(x[5])\n",
    "    \n",
    "def raise_slice(x):\n",
    "    raise ValueError(x[5:7])\n",
    "\n",
    "def raise_step_slice(x):\n",
    "    raise ValueError(x[::-1])\n",
    "    \n",
    "def test_index():\n",
    "    raise_index(mock.MagicMock())\n",
    "\n",
    "def test_slice():\n",
    "    raise_slice(mock.MagicMock())\n",
    "    \n",
    "def test_step_slice():\n",
    "    raise_step_slice(mock.MagicMock())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mocks can be *named*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naming mocks is a good practice. This makes many errors easier to disagnose from the exception or assertion message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.9.0, pytest-6.2.2, py-1.10.0, pluggy-0.13.1 -- /opt/carme/homedir/venvs/testing/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /opt/carme/homedir/src/pycon2021-tutorial-testing\n",
      "collecting ... collected 2 items\n",
      "\n",
      "tmpblf1ov79.py::test_opaque FAILED                                       [ 50%]\n",
      "tmpblf1ov79.py::test_clear FAILED                                        [100%]\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "_________________________________ test_opaque __________________________________\n",
      "\n",
      "    def test_opaque():\n",
      "        source = mock.MagicMock()\n",
      "        target = mock.MagicMock()\n",
      ">       copy_stuff(source, target)\n",
      "\n",
      "<ipython-input-20-4ccbfe67e409>:14: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "<ipython-input-20-4ccbfe67e409>:4: in copy_stuff\n",
      "    write_to(target, source, 10)\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "source = <MagicMock id='140676999674272'>\n",
      "target = <MagicMock id='140676999527104'>, length = 10\n",
      "\n",
      "    def write_to(source, target, length):\n",
      "        stuff = source.read()\n",
      "        target.write(stuff)\n",
      ">       raise ValueError(source, target, stuff)\n",
      "E       ValueError: (<MagicMock id='140676999674272'>, <MagicMock id='140676999527104'>, <MagicMock name='mock.read()' id='140676999169504'>)\n",
      "\n",
      "<ipython-input-20-4ccbfe67e409>:9: ValueError\n",
      "__________________________________ test_clear __________________________________\n",
      "\n",
      "    def test_clear():\n",
      "        source = mock.MagicMock(name=\"source\")\n",
      "        target = mock.MagicMock(name=\"target\")\n",
      ">       copy_stuff(source, target)\n",
      "\n",
      "<ipython-input-20-4ccbfe67e409>:19: \n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "<ipython-input-20-4ccbfe67e409>:4: in copy_stuff\n",
      "    write_to(target, source, 10)\n",
      "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
      "\n",
      "source = <MagicMock name='target' id='140676999220816'>\n",
      "target = <MagicMock name='source' id='140676999232720'>, length = 10\n",
      "\n",
      "    def write_to(source, target, length):\n",
      "        stuff = source.read()\n",
      "        target.write(stuff)\n",
      ">       raise ValueError(source, target, stuff)\n",
      "E       ValueError: (<MagicMock name='target' id='140676999220816'>, <MagicMock name='source' id='140676999232720'>, <MagicMock name='target.read()' id='140676999516416'>)\n",
      "\n",
      "<ipython-input-20-4ccbfe67e409>:9: ValueError\n",
      "=========================== short test summary info ============================\n",
      "FAILED tmpblf1ov79.py::test_opaque - ValueError: (<MagicMock id='140676999674...\n",
      "FAILED tmpblf1ov79.py::test_clear - ValueError: (<MagicMock name='target' id=...\n",
      "============================== 2 failed in 0.06s ===============================\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -vv\n",
    "\n",
    "from unittest import mock\n",
    "\n",
    "def copy_stuff(source, target):\n",
    "    write_to(target, source, 10)\n",
    "    \n",
    "def write_to(source, target, length):\n",
    "    stuff = source.read()\n",
    "    target.write(stuff)\n",
    "    raise ValueError(source, target, stuff)\n",
    "\n",
    "def test_opaque():\n",
    "    source = mock.MagicMock()\n",
    "    target = mock.MagicMock()\n",
    "    copy_stuff(source, target)\n",
    "    \n",
    "def test_clear():\n",
    "    source = mock.MagicMock(name=\"source\")\n",
    "    target = mock.MagicMock(name=\"target\")\n",
    "    copy_stuff(source, target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare \n",
    "\n",
    "```\n",
    "ValueError: (<MagicMock id='140019688468192'>, <MagicMock id='140019688435776'>, <MagicMock name='mock.read()' id='140019688426272'>)\n",
    "```\n",
    "\n",
    "with \n",
    "\n",
    "```\n",
    "ValueError: (<MagicMock name='target' id='140019688082976'>, <MagicMock name='source' id='140019895734528'>, <MagicMock name='target.read()' id='140019895731680'>)\n",
    "```\n",
    "\n",
    "With `target.read()`, the bug is much easier to spot!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting properties and deep properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.9.0, pytest-6.2.2, py-1.10.0, pluggy-0.13.1 -- /opt/carme/homedir/venvs/testing/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /opt/carme/homedir/src/pycon2021-tutorial-testing\n",
      "collecting ... collected 1 item\n",
      "\n",
      "tmph4fn3cr_.py::test_attribute FAILED                                    [100%]\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "________________________________ test_attribute ________________________________\n",
      "\n",
      "    def test_attribute():\n",
      "        x = mock.MagicMock(name=\"thing\")\n",
      "        x.some_attribute = 5\n",
      ">       assert x.some_attribute != 5\n",
      "E       AssertionError: assert 5 != 5\n",
      "E        +  where 5 = <MagicMock name='thing' id='140676998882400'>.some_attribute\n",
      "\n",
      "<ipython-input-22-4633871d00b2>:6: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED tmph4fn3cr_.py::test_attribute - AssertionError: assert 5 != 5\n",
      "============================== 1 failed in 0.02s ===============================\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -vv\n",
    "\n",
    "from unittest import mock\n",
    "\n",
    "def test_attribute():\n",
    "    x = mock.MagicMock(name=\"thing\")\n",
    "    x.some_attribute = 5\n",
    "    assert x.some_attribute != 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also set the attribute in the constructor.\n",
    "This is usually better, because there is no step\n",
    "where the mock object is not correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.9.0, pytest-6.2.2, py-1.10.0, pluggy-0.13.1 -- /opt/carme/homedir/venvs/testing/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /opt/carme/homedir/src/pycon2021-tutorial-testing\n",
      "collecting ... collected 1 item\n",
      "\n",
      "tmplnzote93.py::test_attribute_constructor FAILED                        [100%]\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "__________________________ test_attribute_constructor __________________________\n",
      "\n",
      "    def test_attribute_constructor():\n",
      "        x = mock.MagicMock(name=\"thing\", some_attribute=5)\n",
      ">       assert x.some_attribute != 5\n",
      "E       AssertionError: assert 5 != 5\n",
      "E        +  where 5 = <MagicMock name='thing' id='140676998259280'>.some_attribute\n",
      "\n",
      "<ipython-input-23-e3224abc2017>:5: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED tmplnzote93.py::test_attribute_constructor - AssertionError: assert 5 ...\n",
      "============================== 1 failed in 0.02s ===============================\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -vv\n",
    "\n",
    "from unittest import mock\n",
    "\n",
    "def test_attribute_constructor():\n",
    "    x = mock.MagicMock(name=\"thing\", some_attribute=5)\n",
    "    assert x.some_attribute != 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because by default every attribute on a mock is a mock itself,\n",
    "you can set \"deep attributes\" that violate the \"law\" of Demeter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this context, it's fine -- although some argue that the test\n",
    "is not a \"unit test\" if this is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.9.0, pytest-6.2.2, py-1.10.0, pluggy-0.13.1 -- /opt/carme/homedir/venvs/testing/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /opt/carme/homedir/src/pycon2021-tutorial-testing\n",
      "collecting ... collected 1 item\n",
      "\n",
      "tmpi5fflh4h.py::test_deep_attribute FAILED                               [100%]\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "_____________________________ test_deep_attribute ______________________________\n",
      "\n",
      "    def test_deep_attribute():\n",
      "        x = mock.MagicMock(name=\"thing\")\n",
      "        x.some_attribute.value = 5\n",
      ">       assert x.some_attribute.value != 5\n",
      "E       AssertionError: assert 5 != 5\n",
      "E        +  where 5 = <MagicMock name='thing.some_attribute' id='140676997946768'>.value\n",
      "E        +    where <MagicMock name='thing.some_attribute' id='140676997946768'> = <MagicMock name='thing' id='140676997999920'>.some_attribute\n",
      "\n",
      "<ipython-input-24-c4f98f27d5f6>:6: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED tmpi5fflh4h.py::test_deep_attribute - AssertionError: assert 5 != 5\n",
      "============================== 1 failed in 0.03s ===============================\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -vv\n",
    "\n",
    "from unittest import mock\n",
    "\n",
    "def test_deep_attribute():\n",
    "    x = mock.MagicMock(name=\"thing\")\n",
    "    x.some_attribute.value = 5\n",
    "    assert x.some_attribute.value != 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also set several attributes,\n",
    "deep or otherwise,\n",
    "on a mock at the same time using\n",
    "`configure_mock`.\n",
    "This is no different than using the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.9.0, pytest-6.2.2, py-1.10.0, pluggy-0.13.1 -- /opt/carme/homedir/venvs/testing/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /opt/carme/homedir/src/pycon2021-tutorial-testing\n",
      "collecting ... collected 1 item\n",
      "\n",
      "tmps6kmkl5h.py::test_config_mock FAILED                                  [100%]\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "_______________________________ test_config_mock _______________________________\n",
      "\n",
      "    def test_config_mock():\n",
      "        x = mock.MagicMock(name=\"thing\")\n",
      "        x.configure_mock(**{\n",
      "            \"some_attribute.value\": 5,\n",
      "            \"gauge\": 7,\n",
      "        })\n",
      ">       assert x.some_attribute.value != 5 or x.gauge != 7\n",
      "E       AssertionError: assert (5 != 5 or 7 != 7)\n",
      "E        +  where 5 = <MagicMock name='thing.some_attribute' id='140676998632352'>.value\n",
      "E        +    where <MagicMock name='thing.some_attribute' id='140676998632352'> = <MagicMock name='thing' id='140676997745728'>.some_attribute\n",
      "E        +  and   7 = <MagicMock name='thing' id='140676997745728'>.gauge\n",
      "\n",
      "<ipython-input-25-b5daefe0a747>:9: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED tmps6kmkl5h.py::test_config_mock - AssertionError: assert (5 != 5 or 7...\n",
      "============================== 1 failed in 0.03s ===============================\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -vv\n",
    "\n",
    "from unittest import mock\n",
    "\n",
    "def test_config_mock():\n",
    "    x = mock.MagicMock(name=\"thing\")\n",
    "    x.configure_mock(**{\n",
    "        \"some_attribute.value\": 5,\n",
    "        \"gauge\": 7,\n",
    "    })\n",
    "    assert x.some_attribute.value != 5 or x.gauge != 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise (16:35-16:45)\n",
    "\n",
    "Video will pause for 10 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_pytest[clean] -vv\n",
    "\n",
    "from unittest import mock\n",
    "\n",
    "def add_1(x):\n",
    "    return x.value + 1\n",
    "\n",
    "def test_deep_attribute():\n",
    "    x = mock.MagicMock(name=\"thing\")\n",
    "    pass # Change only this line\n",
    "    assert add_1(x.some_attribute) == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_pytest[clean] -vv\n",
    "\n",
    "from unittest import mock\n",
    "def add_1(x):\n",
    "    return x.value + 1\n",
    "\n",
    "   \n",
    "def test_config_mock():\n",
    "    x = mock.MagicMock(name=\"thing\")\n",
    "    x.configure_mock(**{\n",
    "        # Change only this line\n",
    "        \"gauge\": 7,\n",
    "    })\n",
    "    assert add_1(x.some_attribute) == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.9.0, pytest-6.2.2, py-1.10.0, pluggy-0.13.1 -- /opt/carme/homedir/venvs/testing/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /opt/carme/homedir/src/pycon2021-tutorial-testing\n",
      "collecting ... collected 1 item\n",
      "\n",
      "tmppp9026tm.py::test_deep_attribute PASSED                               [100%]\n",
      "\n",
      "============================== 1 passed in 0.02s ===============================\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -vv\n",
    "\n",
    "from unittest import mock\n",
    "\n",
    "def add_1(x):\n",
    "    return x.value + 1\n",
    "\n",
    "def test_deep_attribute():\n",
    "    x = mock.MagicMock(name=\"thing\")\n",
    "    x.some_attribute.value = 1 # pass # Change only this line\n",
    "    assert add_1(x.some_attribute) == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.9.0, pytest-6.2.2, py-1.10.0, pluggy-0.13.1 -- /opt/carme/homedir/venvs/testing/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /opt/carme/homedir/src/pycon2021-tutorial-testing\n",
      "collecting ... collected 1 item\n",
      "\n",
      "tmp049uj949.py::test_config_mock PASSED                                  [100%]\n",
      "\n",
      "============================== 1 passed in 0.02s ===============================\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -vv\n",
    "\n",
    "from unittest import mock\n",
    "def add_1(x):\n",
    "    return x.value + 1\n",
    "\n",
    "   \n",
    "def test_config_mock():\n",
    "    x = mock.MagicMock(name=\"thing\")\n",
    "    x.configure_mock(**{\n",
    "        \"some_attribute.value\": 1, # Change only this line\n",
    "        \"gauge\": 7,\n",
    "    })\n",
    "    assert add_1(x.some_attribute) == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "* Name mocks\n",
    "* Set properties and \"deep\" properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Break (16:55 - 17:05)\n",
    "\n",
    "Video will pause for 10 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Mocks (17:05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mock return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python's duck-typing means that often what we want to do with mock objects is *call them* (or *call methods on them*) and get specific return values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A mock object returns whatever value is in its `return_value` attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like any attribute, a mock object will have that attribute,\n",
    "and it will be a mock by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.9.0, pytest-6.2.2, py-1.10.0, pluggy-0.13.1 -- /opt/carme/homedir/venvs/testing/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /opt/carme/homedir/src/pycon2021-tutorial-testing\n",
      "collecting ... collected 1 item\n",
      "\n",
      "tmp5s6p0evt.py::test_use_return_value FAILED                             [100%]\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "____________________________ test_use_return_value _____________________________\n",
      "\n",
      "    def test_use_return_value():\n",
      "        obj = mock.MagicMock(name=\"obj\")\n",
      "        obj.return_value.some_attribute = 5\n",
      ">       assert obj().some_attribute != 5\n",
      "E       AssertionError: assert 5 != 5\n",
      "E        +  where 5 = <MagicMock name='obj()' id='140116867237440'>.some_attribute\n",
      "E        +    where <MagicMock name='obj()' id='140116867237440'> = <MagicMock name='obj' id='140116871184880'>()\n",
      "\n",
      "<ipython-input-8-261c06bcebf3>:6: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED tmp5s6p0evt.py::test_use_return_value - AssertionError: assert 5 != 5\n",
      "============================== 1 failed in 0.03s ===============================\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -vv\n",
    "\n",
    "from unittest import mock\n",
    "\n",
    "def test_use_return_value():\n",
    "    obj = mock.MagicMock(name=\"obj\")\n",
    "    obj.return_value.some_attribute = 5\n",
    "    assert obj().some_attribute != 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in other cases of \"deep attributes\", you can set it in the constructor with the right \"keyword arguments\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.9.0, pytest-6.2.2, py-1.10.0, pluggy-0.13.1 -- /opt/carme/homedir/venvs/testing/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /opt/carme/homedir/src/pycon2021-tutorial-testing\n",
      "collecting ... collected 1 item\n",
      "\n",
      "tmpzhdm4gcl.py::test_use_return_value_constructor FAILED                 [100%]\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "______________________ test_use_return_value_constructor _______________________\n",
      "\n",
      "    def test_use_return_value_constructor():\n",
      "        obj = mock.MagicMock(name=\"obj\", **{\"return_value.some_attribute\": 5})\n",
      ">       assert obj().some_attribute != 5\n",
      "E       AssertionError: assert 5 != 5\n",
      "E        +  where 5 = <MagicMock name='obj()' id='140116866548352'>.some_attribute\n",
      "E        +    where <MagicMock name='obj()' id='140116866548352'> = <MagicMock name='obj' id='140116866532208'>()\n",
      "\n",
      "<ipython-input-9-ec29521bde2c>:5: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED tmpzhdm4gcl.py::test_use_return_value_constructor - AssertionError: as...\n",
      "============================== 1 failed in 0.02s ===============================\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -vv\n",
    "\n",
    "from unittest import mock\n",
    "\n",
    "def test_use_return_value_constructor():\n",
    "    obj = mock.MagicMock(name=\"obj\", **{\"return_value.some_attribute\": 5})\n",
    "    assert obj().some_attribute != 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can set the `return_value` property itself, which will return a regular value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.9.0, pytest-6.2.2, py-1.10.0, pluggy-0.13.1 -- /opt/carme/homedir/venvs/testing/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /opt/carme/homedir/src/pycon2021-tutorial-testing\n",
      "collecting ... collected 1 item\n",
      "\n",
      "tmpvgolblit.py::test_set_return_value FAILED                             [100%]\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "____________________________ test_set_return_value _____________________________\n",
      "\n",
      "    def test_set_return_value():\n",
      "        obj = mock.MagicMock(name=\"obj\")\n",
      "        obj.return_value = 5\n",
      ">       assert obj() != 5\n",
      "E       AssertionError: assert 5 != 5\n",
      "E        +  where 5 = <MagicMock name='obj' id='140116866268944'>()\n",
      "\n",
      "<ipython-input-10-288012d5b6af>:6: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED tmpvgolblit.py::test_set_return_value - AssertionError: assert 5 != 5\n",
      "============================== 1 failed in 0.02s ===============================\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -vv\n",
    "\n",
    "from unittest import mock\n",
    "\n",
    "def test_set_return_value():\n",
    "    obj = mock.MagicMock(name=\"obj\")\n",
    "    obj.return_value = 5\n",
    "    assert obj() != 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can set it using the constructor too. Here, a regular keyword argument will work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.9.0, pytest-6.2.2, py-1.10.0, pluggy-0.13.1 -- /opt/carme/homedir/venvs/testing/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /opt/carme/homedir/src/pycon2021-tutorial-testing\n",
      "collecting ... collected 1 item\n",
      "\n",
      "tmp9lhg6q_f.py::test_set_return_value_constructor FAILED                 [100%]\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "______________________ test_set_return_value_constructor _______________________\n",
      "\n",
      "    def test_set_return_value_constructor():\n",
      "        obj = mock.MagicMock(name=\"obj\", return_value=5)\n",
      ">       assert obj() != 5\n",
      "E       AssertionError: assert 5 != 5\n",
      "E        +  where 5 = <MagicMock name='obj' id='140116868026720'>()\n",
      "\n",
      "<ipython-input-11-41e117e8dd61>:5: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED tmp9lhg6q_f.py::test_set_return_value_constructor - AssertionError: as...\n",
      "============================== 1 failed in 0.02s ===============================\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -vv\n",
    "\n",
    "from unittest import mock\n",
    "\n",
    "def test_set_return_value_constructor():\n",
    "    obj = mock.MagicMock(name=\"obj\", return_value=5)\n",
    "    assert obj() != 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common occurence is wanting to set the return value of a *method*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.9.0, pytest-6.2.2, py-1.10.0, pluggy-0.13.1 -- /opt/carme/homedir/venvs/testing/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /opt/carme/homedir/src/pycon2021-tutorial-testing\n",
      "collecting ... collected 1 item\n",
      "\n",
      "tmpvne3_zll.py::test_set_rmethod_return_value FAILED                     [100%]\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "________________________ test_set_rmethod_return_value _________________________\n",
      "\n",
      "    def test_set_rmethod_return_value():\n",
      "        obj = mock.MagicMock(name=\"obj\")\n",
      "        obj.method.return_value = 5\n",
      ">       assert obj.method() != 5\n",
      "E       AssertionError: assert 5 != 5\n",
      "E        +  where 5 = <MagicMock name='obj.method' id='140116865713872'>()\n",
      "E        +    where <MagicMock name='obj.method' id='140116865713872'> = <MagicMock name='obj' id='140116866209776'>.method\n",
      "\n",
      "<ipython-input-12-bcd77aafe477>:6: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED tmpvne3_zll.py::test_set_rmethod_return_value - AssertionError: assert...\n",
      "============================== 1 failed in 0.03s ===============================\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -vv\n",
    "\n",
    "from unittest import mock\n",
    "\n",
    "def test_set_rmethod_return_value():\n",
    "    obj = mock.MagicMock(name=\"obj\")\n",
    "    obj.method.return_value = 5\n",
    "    assert obj.method() != 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the return value of a method is a \"deep attribute\", so you will need to use the special syntax\n",
    "to pass it in the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.9.0, pytest-6.2.2, py-1.10.0, pluggy-0.13.1 -- /opt/carme/homedir/venvs/testing/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /opt/carme/homedir/src/pycon2021-tutorial-testing\n",
      "collecting ... collected 1 item\n",
      "\n",
      "tmpkd3fpmoj.py::test_set_rmethod_return_value_constructor FAILED         [100%]\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "__________________ test_set_rmethod_return_value_constructor ___________________\n",
      "\n",
      "    def test_set_rmethod_return_value_constructor():\n",
      "        obj = mock.MagicMock(name=\"obj\", **{\"method.return_value\": 5})\n",
      ">       assert obj.method() != 5\n",
      "E       AssertionError: assert 5 != 5\n",
      "E        +  where 5 = <MagicMock name='obj.method' id='140116529792240'>()\n",
      "E        +    where <MagicMock name='obj.method' id='140116529792240'> = <MagicMock name='obj' id='140116529776096'>.method\n",
      "\n",
      "<ipython-input-13-cfa63262ff22>:5: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED tmpkd3fpmoj.py::test_set_rmethod_return_value_constructor - AssertionE...\n",
      "============================== 1 failed in 0.03s ===============================\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -vv\n",
    "\n",
    "from unittest import mock\n",
    "\n",
    "def test_set_rmethod_return_value_constructor():\n",
    "    obj = mock.MagicMock(name=\"obj\", **{\"method.return_value\": 5})\n",
    "    assert obj.method() != 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting all of these ideas together, in order to set an *attribute* on the return value of a *method*,\n",
    "you will have to use the special syntax, and have *two* dots in the name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================= test session starts ==============================\n",
      "platform linux -- Python 3.9.0, pytest-6.2.2, py-1.10.0, pluggy-0.13.1 -- /opt/carme/homedir/venvs/testing/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /opt/carme/homedir/src/pycon2021-tutorial-testing\n",
      "collecting ... collected 1 item\n",
      "\n",
      "tmpuih_snfy.py::test_set_rmethod_deep_return_value_constructor FAILED    [100%]\n",
      "\n",
      "=================================== FAILURES ===================================\n",
      "________________ test_set_rmethod_deep_return_value_constructor ________________\n",
      "\n",
      "    def test_set_rmethod_deep_return_value_constructor():\n",
      "        obj = mock.MagicMock(name=\"obj\", **{\"method.return_value.some_attribute\": 5})\n",
      ">       assert obj.method().some_attribute != 5\n",
      "E       AssertionError: assert 5 != 5\n",
      "E        +  where 5 = <MagicMock name='obj.method()' id='140116865906768'>.some_attribute\n",
      "E        +    where <MagicMock name='obj.method()' id='140116865906768'> = <MagicMock name='obj.method' id='140116529546816'>()\n",
      "E        +      where <MagicMock name='obj.method' id='140116529546816'> = <MagicMock name='obj' id='140116868822448'>.method\n",
      "\n",
      "<ipython-input-14-446802117931>:5: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED tmpuih_snfy.py::test_set_rmethod_deep_return_value_constructor - Asser...\n",
      "============================== 1 failed in 0.02s ===============================\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean] -vv\n",
    "\n",
    "from unittest import mock\n",
    "\n",
    "def test_set_rmethod_deep_return_value_constructor():\n",
    "    obj = mock.MagicMock(name=\"obj\", **{\"method.return_value.some_attribute\": 5})\n",
    "    assert obj.method().some_attribute != 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mock side effect -- iterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "One of the things that can be assigned to `side_effect` is\n",
    "an *iterable*, such as a sequence or a generator.\n",
    "\n",
    "This is a powerful feature -- it allows controlling each call's return value,\n",
    "with little code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F                                                                        [100%]\n",
      "=================================== FAILURES ===================================\n",
      "_________________________________ test_values __________________________________\n",
      "\n",
      "    def test_values():\n",
      "        different_things = mock.MagicMock()\n",
      "        different_things.side_effect = [1, 2, 3]\n",
      "        assert different_things() == 1\n",
      "        assert different_things() == 2\n",
      ">       assert different_things() == 4\n",
      "E       AssertionError: assert 3 == 4\n",
      "E        +  where 3 = <MagicMock id='140116529410976'>()\n",
      "\n",
      "<ipython-input-15-0d16f591cdaa>:8: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED tmppli2flos.py::test_values - AssertionError: assert 3 == 4\n",
      "1 failed in 0.02s\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "from unittest import mock\n",
    "\n",
    "def test_values():\n",
    "    different_things = mock.MagicMock()\n",
    "    different_things.side_effect = [1, 2, 3]\n",
    "    assert different_things() == 1\n",
    "    assert different_things() == 2\n",
    "    assert different_things() == 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more realistic example is when simulating file input.\n",
    "In this case, we want to be able to control what `readline` returns\n",
    "each time to pretend it is file input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F                                                                        [100%]\n",
      "=================================== FAILURES ===================================\n",
      "_________________________________ test_parser __________________________________\n",
      "\n",
      "    def test_parser():\n",
      "        filelike = mock.MagicMock(spec=TextIOBase)\n",
      "        filelike.readline.side_effect = [\n",
      "            \"thing important\\n\",\n",
      "            \"a-little\\n\",\n",
      "            \"to-some-people\\n\"\n",
      "        ]\n",
      "        value = parse_three_lines(filelike)\n",
      ">       assert value == dict(thing=\"important/a-little+to-most-people\")\n",
      "E       AssertionError: assert {'thing': 'im...-some-people'} == {'thing': 'im...-most-people'}\n",
      "E         Differing items:\n",
      "E         {'thing': 'important/a-little+to-some-people'} != {'thing': 'important/a-little+to-most-people'}\n",
      "E         Full diff:\n",
      "E         - {'thing': 'important/a-little+to-most-people'}\n",
      "E         ?                                   ^^^\n",
      "E         + {'thing': 'important/a-little+to-some-people'}\n",
      "E         ?                                  ++ ^\n",
      "\n",
      "<ipython-input-16-fadfe3d548ab>:20: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED tmpkjp6blny.py::test_parser - AssertionError: assert {'thing': 'im...-...\n",
      "1 failed in 0.02s\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "from unittest import mock\n",
    "\n",
    "def parse_three_lines(fpin):\n",
    "    line = fpin.readline()\n",
    "    name, value = line.split()\n",
    "    modifier = fpin.readline().strip()\n",
    "    extra = fpin.readline().strip()\n",
    "    return {name: f\"{value}/{modifier}+{extra}\"}\n",
    "\n",
    "from io import TextIOBase\n",
    "    \n",
    "def test_parser():\n",
    "    filelike = mock.MagicMock(spec=TextIOBase)\n",
    "    filelike.readline.side_effect = [\n",
    "        \"thing important\\n\",\n",
    "        \"a-little\\n\",\n",
    "        \"to-some-people\\n\"\n",
    "    ]\n",
    "    value = parse_three_lines(filelike)\n",
    "    assert value == dict(thing=\"important/a-little+to-most-people\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mock side effect -- function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, the above example was simplified: real network service test code should verify that the results it got were correct to validate that the server works correctly. This means doing a synthetic request and looking for a correct result. The mock object has to emulate that. It has to perform some computation on the inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to test such code without performing any computation is difficult. The tests tend to be too *insensitive* or too *flakey*. An insensitive test is one that does not fail in the presence of bugs. A flakey test is one that sometimes fails, even when the code is  correct. Here, our code is incorrect. The insensitive test does not catch it, while the flakey test would fail even if it was fixed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".F...F                                                                   [100%]\n",
      "=================================== FAILURES ===================================\n",
      "_____________________________ test_flakey_test[1] ______________________________\n",
      "\n",
      "does_nothing = 1\n",
      "\n",
      "    @pytest.mark.parametrize(\"does_nothing\", [1, 2, 3, 4, 5])\n",
      "    def test_flakey_test(does_nothing):\n",
      "        sock = mock.MagicMock(spec=socket.socket)\n",
      "        sock.makefile.return_value.readline.side_effect = [\"key\\n\", \"value\\n\"]\n",
      ">       assert yolo_reader(sock) == {\"key\": \"value\"}\n",
      "E       AssertionError: assert {'value': 'key'} == {'key': 'value'}\n",
      "E         Left contains 1 more item:\n",
      "E         {'value': 'key'}\n",
      "E         Right contains 1 more item:\n",
      "E         {'key': 'value'}\n",
      "E         Full diff:\n",
      "E         - {'key': 'value'}\n",
      "E         + {'value': 'key'}\n",
      "\n",
      "<ipython-input-19-c0775702ebb3>:32: AssertionError\n",
      "_____________________________ test_flakey_test[5] ______________________________\n",
      "\n",
      "does_nothing = 5\n",
      "\n",
      "    @pytest.mark.parametrize(\"does_nothing\", [1, 2, 3, 4, 5])\n",
      "    def test_flakey_test(does_nothing):\n",
      "        sock = mock.MagicMock(spec=socket.socket)\n",
      "        sock.makefile.return_value.readline.side_effect = [\"key\\n\", \"value\\n\"]\n",
      ">       assert yolo_reader(sock) == {\"key\": \"value\"}\n",
      "E       AssertionError: assert {'value': 'key'} == {'key': 'value'}\n",
      "E         Left contains 1 more item:\n",
      "E         {'value': 'key'}\n",
      "E         Right contains 1 more item:\n",
      "E         {'key': 'value'}\n",
      "E         Full diff:\n",
      "E         - {'key': 'value'}\n",
      "E         + {'value': 'key'}\n",
      "\n",
      "<ipython-input-19-c0775702ebb3>:32: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED tmpgtopaz50.py::test_flakey_test[1] - AssertionError: assert {'value':...\n",
      "FAILED tmpgtopaz50.py::test_flakey_test[5] - AssertionError: assert {'value':...\n",
      "2 failed, 4 passed in 0.07s\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean]\n",
    "import socket\n",
    "import random\n",
    "\n",
    "def yolo_reader(sock):\n",
    "    sock.settimeout(5)\n",
    "    sock.connect((\"some.host\", 8451))\n",
    "    fpin = sock.makefile()\n",
    "    order = [0, 1]\n",
    "    random.shuffle(order)\n",
    "    while order:\n",
    "        if order.pop() == 0:\n",
    "            sock.sendall(b\"GET KEY\\n\")\n",
    "            key = fpin.readline().strip()\n",
    "        else:\n",
    "            sock.sendall(b\"GET VALUE\\n\")\n",
    "            value = fpin.readline().strip()\n",
    "    return {value: key} ## Woops bug, should be {key: value}\n",
    "    \n",
    "from io import TextIOBase\n",
    "from unittest import mock\n",
    "import pytest\n",
    "\n",
    "def test_insensitive_test():\n",
    "    sock = mock.MagicMock(spec=socket.socket)\n",
    "    sock.makefile.return_value.readline.return_value = \"interesting\\n\"\n",
    "    assert yolo_reader(sock) == {\"interesting\": \"interesting\"}\n",
    "    \n",
    "@pytest.mark.parametrize(\"does_nothing\", [1, 2, 3, 4, 5])\n",
    "def test_flakey_test(does_nothing):\n",
    "    sock = mock.MagicMock(spec=socket.socket)\n",
    "    sock.makefile.return_value.readline.side_effect = [\"key\\n\", \"value\\n\"]\n",
    "    assert yolo_reader(sock) == {\"key\": \"value\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final option of getting results from a mock object is to assign a *callable object* to `side_effect`. This calls `side_effect` to simply call it. Why not just assign a callable object directly to the attribute? Have patience, we'll get to that in the next part!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, our callable object (just a function) will assign a `return_value` to the attribute of another object. This is not that uncommon. We are simulating the environment, and in a real environment, poking one thing often has an effect on other things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F                                                                        [100%]\n",
      "=================================== FAILURES ===================================\n",
      "________________________________ test_yolo_well ________________________________\n",
      "\n",
      "    def test_yolo_well():\n",
      "        sock = mock.MagicMock(spec=socket.socket)\n",
      "        def sendall(data):\n",
      "            cmd, name = data.decode(\"ascii\").split()\n",
      "            if name == \"KEY\":\n",
      "                sock.makefile.return_value.readline.return_value = \"key\\n\"\n",
      "            elif name == \"VALUE\":\n",
      "                sock.makefile.return_value.readline.return_value = \"value\\n\"\n",
      "            else:\n",
      "                raise ValueError(\"got bad command\", name)\n",
      "        sock.sendall.side_effect = sendall\n",
      ">       assert yolo_reader(sock) == {\"key\": \"value\"}\n",
      "E       AssertionError: assert {'value': 'key'} == {'key': 'value'}\n",
      "E         Left contains 1 more item:\n",
      "E         {'value': 'key'}\n",
      "E         Right contains 1 more item:\n",
      "E         {'key': 'value'}\n",
      "E         Full diff:\n",
      "E         - {'key': 'value'}\n",
      "E         + {'value': 'key'}\n",
      "\n",
      "<ipython-input-24-2466a9c67c5b>:33: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED tmp_0czby6j.py::test_yolo_well - AssertionError: assert {'value': 'key...\n",
      "1 failed in 0.02s\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean]\n",
    "import socket\n",
    "import random\n",
    "\n",
    "def yolo_reader(sock):\n",
    "    sock.settimeout(5)\n",
    "    sock.connect((\"some.host\", 8451))\n",
    "    fpin = sock.makefile()\n",
    "    order = [0, 1]\n",
    "    random.shuffle(order)\n",
    "    while order:\n",
    "        if order.pop() == 0:\n",
    "            sock.sendall(b\"GET KEY\\n\")\n",
    "            key = fpin.readline().strip()\n",
    "        else:\n",
    "            sock.sendall(b\"GET VALUE\\n\")\n",
    "            value = fpin.readline().strip()\n",
    "    return {value: key} ## Woops bug, should be {key: value}\n",
    "    \n",
    "from io import TextIOBase\n",
    "from unittest import mock\n",
    "\n",
    "def test_yolo_well():\n",
    "    sock = mock.MagicMock(spec=socket.socket)\n",
    "    def sendall(data):\n",
    "        cmd, name = data.decode(\"ascii\").split()\n",
    "        if name == \"KEY\":\n",
    "            sock.makefile.return_value.readline.return_value = \"key\\n\"\n",
    "        elif name == \"VALUE\":\n",
    "            sock.makefile.return_value.readline.return_value = \"value\\n\"\n",
    "        else:\n",
    "            raise ValueError(\"got bad command\", name)\n",
    "    sock.sendall.side_effect = sendall\n",
    "    assert yolo_reader(sock) == {\"key\": \"value\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mock call args and call args list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Call arguments\n",
    "\n",
    "In the following example, we want to make sure the code calls the method with the *correct* arguments.\n",
    "When automating data center manipulations, it is important to get things right.\n",
    "As they say, \"To err is human, but to destroy an entire data center requires a robot with a bug.\"\n",
    "\n",
    "We want to make sure our Paramiko-based automation will correctly get the sizes of files, even when the file names have spaces in them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".F                                                                       [100%]\n",
      "=================================== FAILURES ===================================\n",
      "____________________________ test_file_size[a file] ____________________________\n",
      "\n",
      "fname = 'a file'\n",
      "\n",
      "    @pytest.mark.parametrize(\"fname\", [\"readme.txt\", \"a file\"])\n",
      "    def test_file_size(fname):\n",
      "        client = mock.MagicMock()\n",
      "        client.exec_command.return_value = [mock.MagicMock(name=str(i)) for i in range(3)]\n",
      "        client.exec_command.return_value[1].read.return_value = f\"\"\"\\\n",
      "        -rw-rw-r--  1 user user    123 Jul 18 20:25 {fname}\n",
      "        \"\"\"\n",
      "        client.exec_command.return_value[2].read.return_value = \"\"\n",
      "        result = get_remote_file_size(client, fname)\n",
      "        assert result == 123\n",
      "        [args], kwargs = client.exec_command.call_args\n",
      ">       assert shlex.split(args) == [\"ls\", \"-l\", fname]\n",
      "E       AssertionError: assert ['ls', '-l', 'a', 'file'] == ['ls', '-l', 'a file']\n",
      "E         At index 2 diff: 'a' != 'a file'\n",
      "E         Left contains one more item: 'file'\n",
      "E         Full diff:\n",
      "E         - ['ls', '-l', 'a file']\n",
      "E         ?                ^\n",
      "E         + ['ls', '-l', 'a', 'file']\n",
      "E         ?                ^^^^\n",
      "\n",
      "<ipython-input-27-ff77804c1087>:28: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED tmpvpmu39ca.py::test_file_size[a file] - AssertionError: assert ['ls',...\n",
      "1 failed, 1 passed in 0.04s\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "def get_remote_file_size(client, fname):\n",
    "    client.connect('ssh.example.com')\n",
    "    stdin, stdout, stderr = client.exec_command(f\"ls -l {fname}\")\n",
    "    stdin.close()\n",
    "    results = stdout.read()\n",
    "    errors = stderr.read()\n",
    "    stdout.close()\n",
    "    stderr.close()\n",
    "    if errors != '':\n",
    "        raise ValueError(\"problem with command\", errors)\n",
    "    return int(results.split()[4])\n",
    "\n",
    "import pytest\n",
    "from unittest import mock\n",
    "import shlex\n",
    "\n",
    "@pytest.mark.parametrize(\"fname\", [\"readme.txt\", \"a file\"])\n",
    "def test_file_size(fname):\n",
    "    client = mock.MagicMock()\n",
    "    client.exec_command.return_value = [mock.MagicMock(name=str(i)) for i in range(3)]\n",
    "    client.exec_command.return_value[1].read.return_value = f\"\"\"\\\n",
    "    -rw-rw-r--  1 user user    123 Jul 18 20:25 {fname}\n",
    "    \"\"\"\n",
    "    client.exec_command.return_value[2].read.return_value = \"\"\n",
    "    result = get_remote_file_size(client, fname)\n",
    "    assert result == 123\n",
    "    [args], kwargs = client.exec_command.call_args\n",
    "    assert shlex.split(args) == [\"ls\", \"-l\", fname]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List of call args\n",
    "\n",
    "Sometimes, this is not enough. Some code calls functions repeatedly, and we need to test that *all* calls are correct.\n",
    "The most sophisticated X-Ray we have is `.call_args_list` which gives the entire history of what happened to the callable.\n",
    "\n",
    "For this example, we will pretend that the (*real*) remote calculator API only allows multiplying two numbers. In order to *cube* the number, calculate `x**3`, we need two calls to the service. For superstitious reasons, we want to always put the bigger number first: maybe someone told us that it is faster this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F                                                                        [100%]\n",
      "=================================== FAILURES ===================================\n",
      "_____________________________ test_calculate_cube ______________________________\n",
      "\n",
      "    def test_calculate_cube():\n",
      "        client = mock.MagicMock(spec=httpx.Client)\n",
      "        client.get.side_effect = [mock.MagicMock(text=str(x)) for x in [25, 125]]\n",
      "        assert calculate_cube(client, 5) == 125\n",
      "        assert client.get.call_count == 2\n",
      "        squaring, cubing = client.get.call_args_list\n",
      "        args, kwargs = squaring\n",
      "        assert kwargs == {}\n",
      "        assert args == tuple([\"https://api.mathjs.org/v4/?expr=5*5\"])\n",
      "        args, kwargs = cubing\n",
      "        assert kwargs == {}\n",
      "        ## Make sure bigger number comes first!\n",
      ">       assert args == tuple([\"https://api.mathjs.org/v4/?expr=25*5\"])\n",
      "E       AssertionError: assert ('https://api.../?expr=5*25',) == ('https://api.../?expr=25*5',)\n",
      "E         At index 0 diff: 'https://api.mathjs.org/v4/?expr=5*25' != 'https://api.mathjs.org/v4/?expr=25*5'\n",
      "E         Full diff:\n",
      "E         - ('https://api.mathjs.org/v4/?expr=25*5',)\n",
      "E         ?                                   -\n",
      "E         + ('https://api.mathjs.org/v4/?expr=5*25',)\n",
      "E         ?                                     +\n",
      "\n",
      "<ipython-input-30-ac06f1810d94>:20: AssertionError\n",
      "=========================== short test summary info ============================\n",
      "FAILED tmpdk8sx3tw.py::test_calculate_cube - AssertionError: assert ('https:/...\n",
      "1 failed in 0.04s\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "import httpx\n",
    "from unittest import mock\n",
    "\n",
    "def calculate_cube(client, base):\n",
    "    square = int(client.get(f\"https://api.mathjs.org/v4/?expr={base}*{base}\").text) # x*x\n",
    "    return int(client.get(f\"https://api.mathjs.org/v4/?expr={base}*{square}\").text) # x*x*x\n",
    "\n",
    "def test_calculate_cube():\n",
    "    client = mock.MagicMock(spec=httpx.Client)\n",
    "    client.get.side_effect = [mock.MagicMock(text=str(x)) for x in [25, 125]]\n",
    "    assert calculate_cube(client, 5) == 125\n",
    "    assert client.get.call_count == 2\n",
    "    squaring, cubing = client.get.call_args_list\n",
    "    args, kwargs = squaring\n",
    "    assert kwargs == {}\n",
    "    assert args == tuple([\"https://api.mathjs.org/v4/?expr=5*5\"])\n",
    "    args, kwargs = cubing\n",
    "    assert kwargs == {}\n",
    "    ## Make sure bigger number comes first!\n",
    "    assert args == tuple([\"https://api.mathjs.org/v4/?expr=25*5\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise (17:10-17:20)\n",
    "\n",
    "Video will pause for 10 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "import httpx\n",
    "from unittest import mock\n",
    "import pytest\n",
    "import random\n",
    "\n",
    "def calculate_fifth_power(client, base):\n",
    "    square = int(client.get(f\"https://api.mathjs.org/v4/?expr={base}*{base}\").text)\n",
    "    hypercube = int(client.get(f\"https://api.mathjs.org/v4/?expr={square}*{square}\").text)\n",
    "    # random order\n",
    "    args = [hypercube, base]\n",
    "    random.shuffle(args)\n",
    "    result = int(client.get(f\"https://api.mathjs.org/v4/?expr={args[0]}*{args[1]}\").text)\n",
    "    return result\n",
    "\n",
    "@pytest.mark.parametrize(\"does_nothing\", [1, 2, 3, 4, 5])\n",
    "def test_calculate_cube(does_nothing):\n",
    "    client = mock.MagicMock(spec=httpx.Client)\n",
    "    client.get.side_effect = [mock.MagicMock(text=str(x)) for x in [25, 625, 3125]]\n",
    "    assert calculate_fifth_power(client, 5) == 3125\n",
    "    assert client.get.call_count == 3\n",
    "    squaring, hypercubing, final = client.get.call_args_list\n",
    "    args, kwargs = squaring\n",
    "    assert kwargs == {}\n",
    "    assert args == tuple([\"https://api.mathjs.org/v4/?expr=5*5\"])\n",
    "    args, kwargs = hypercubing\n",
    "    assert kwargs == {}\n",
    "    args, kwargs = final\n",
    "    assert kwargs == {}\n",
    "    [url] = args\n",
    "    constant, expr = url.split(\"=\", 1)\n",
    "    assert constant == \"https://api.mathjs.org/v4/?expr\"\n",
    "    assert expr == \"625*5\" # Change only this line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving Exercise (17:20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....                                                                    [100%]\n",
      "5 passed in 0.04s\n"
     ]
    }
   ],
   "source": [
    "%%run_pytest[clean]\n",
    "\n",
    "import httpx\n",
    "from unittest import mock\n",
    "import pytest\n",
    "import random\n",
    "\n",
    "def calculate_fifth_power(client, base):\n",
    "    square = int(client.get(f\"https://api.mathjs.org/v4/?expr={base}*{base}\").text)\n",
    "    hypercube = int(client.get(f\"https://api.mathjs.org/v4/?expr={square}*{square}\").text)\n",
    "    # random order\n",
    "    args = [hypercube, base]\n",
    "    random.shuffle(args)\n",
    "    result = int(client.get(f\"https://api.mathjs.org/v4/?expr={args[0]}*{args[1]}\").text)\n",
    "    return result\n",
    "\n",
    "@pytest.mark.parametrize(\"does_nothing\", [1, 2, 3, 4, 5])\n",
    "def test_calculate_cube(does_nothing):\n",
    "    client = mock.MagicMock(spec=httpx.Client)\n",
    "    client.get.side_effect = [mock.MagicMock(text=str(x)) for x in [25, 625, 3125]]\n",
    "    assert calculate_fifth_power(client, 5) == 3125\n",
    "    assert client.get.call_count == 3\n",
    "    squaring, hypercubing, final = client.get.call_args_list\n",
    "    args, kwargs = squaring\n",
    "    assert kwargs == {}\n",
    "    assert args == tuple([\"https://api.mathjs.org/v4/?expr=5*5\"])\n",
    "    args, kwargs = hypercubing\n",
    "    assert kwargs == {}\n",
    "    args, kwargs = final\n",
    "    assert kwargs == {}\n",
    "    [url] = args\n",
    "    constant, expr = url.split(\"=\", 1)\n",
    "    assert constant == \"https://api.mathjs.org/v4/?expr\"\n",
    "    assert set(expr.split(\"*\")) == {\"625\", \"5\"} # Change only this line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final thoughts and Q&A (17:25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Assertions let you verify conditions\n",
    "* Mocks let you avoid interacting with \"real life\"\n",
    "* You can set mock attributes, return values, or even behavior\n",
    "* Mocks record arguments they were called with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testable code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This shows how to test -- but writing testable code is important!\n",
    "* The power of mocks shines when passing arguments to functions or initializers -- do it more.\n",
    "* Patching is a tool of last resort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testing",
   "language": "python",
   "name": "testing-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
